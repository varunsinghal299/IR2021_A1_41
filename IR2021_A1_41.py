# -*- coding: utf-8 -*-
"""IR_Ass1_By_Varun_Sachin_Updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1108VHmvObFcFSriQ-2nfZDAk__2iUMxe
"""

# mount the drive
from google.colab import drive
drive.mount('/gdrive')

# files_path = '/content/drive/MyDrive/IR_CSE508/Assignment1/stories/'
# read_files = glob.glob(os.path.join(files_path,"*.*"))


import numpy as np     #import numpy libraries ans np
import pandas as pd     #import pandas libraries ans pd

import glob     #the glob module is used to retrieve files/pathnames matching a specified pattern

import nltk   # Python package for natural language processing
nltk.download('stopwords')  # download stopwords
nltk.download('punkt')
nltk.download('wordnet')

#import nltk stopword word_tokenizer nad PortStemmer and WordNetlemmatizer.
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

from nltk.stem import WordNetLemmatizer

import os      # import the os module to interact with the underlying operating system.
import string
import copy
import pandas as pd    # parsing multiple file formats to converting an entire data table into a NumPy matrix array.
import pickle



def convert_lower_case(textdata):   #convert text inro lower case.
    return np.char.lower(textdata)

def remove_stop_words(textdata):  #remove stop word from text data and store into next_text.
    stop_words = stopwords.words('english')
    words = word_tokenize(str(textdata))
    new_text = ""
    for w in words:
        if w not in stop_words:
            new_text = new_text + " " + w
    return np.char.strip(new_text)      #return textdata after removing stopwords.


def remove_punctuation(textdata):   #remove punctuation from textdata.
    symbols = "/:;!\"#+-.&()*^_`<=>?@[\]$%{|}~\n"
    for i in range(len(symbols)):
        textdata = np.char.replace(textdata, symbols[i], ' ')
        textdata = np.char.replace(textdata, "  ", " ")
    textdata = np.char.replace(textdata, ',', ' ')
    textdata = np.char.replace(textdata, "'", "")
    return textdata


def remove_single_characters(textdata): #this function remove single character from textdata and return updated text data.
    words = word_tokenize(str(textdata))
    new_text = ""
    for w in words:
        if len(w) > 1:
            new_text = new_text + " " + w
    return np.char.strip(new_text)   


def convert_numbers(textdata):
    textdata = np.char.replace(textdata, "0", " zero ")     #replace 0 by zero in textdata.
    textdata = np.char.replace(textdata, "1", " one ")      #replace 1 by one in textdata.
    textdata = np.char.replace(textdata, "2", " two ")      #replace 2 by two in textdata.
    textdata = np.char.replace(textdata, "3", " three ")    #replace 3 by three in textdata.
    textdata = np.char.replace(textdata, "4", " four ")     #replace 4 by four in textdata.
    textdata = np.char.replace(textdata, "5", " five ")     #replace 5 by five in textdata.
    textdata = np.char.replace(textdata, "6", " six ")      #replace 6 by six in textdata.
    textdata = np.char.replace(textdata, "7", " seven ")    #replace 7 by seven in textdata.
    textdata = np.char.replace(textdata, "8", " eight ")    #replace 8 by eight in textdata.
    textdata = np.char.replace(textdata, "9", " nine ")     #replace 9 by nine in textdata.
    return textdata     


def stemming(textdata):     #take each word from textdata and then after perform stemming return updated textdata. 
    stemmer= PorterStemmer()
    
    tokens = word_tokenize(str(textdata))
    new_text = ""
    for w in tokens:
        new_text = new_text + " " + stemmer.stem(w)
    return np.char.strip(new_text) 


def lemmatization(textdata):     #take each word from textdata and then after perform lemmatization return updated textdata. 
    lemmatizer=WordNetLemmatizer()
    
    tokens = word_tokenize(str(textdata))
    new_text = ""
    for w in tokens:
        new_text = new_text + " " + lemmatizer.lemmatize(w)
    return np.char.strip(new_text)


def preprocess(textdata):       
    textdata = convert_lower_case(textdata)         #function convert all textdata word into lowercase.
    textdata = convert_numbers(textdata)            #function convert digitwhich are in textdata word into thier corresponding english word.
    textdata = remove_punctuation(textdata)         #function remove punctuation from textdata.
    textdata = remove_stop_words(textdata)          #function remove stop words from textdata.
    textdata = remove_single_characters(textdata)   #function remove single character from textdata.
    # textdata = stemming(textdata)
    textdata = lemmatization(textdata)              #function perform lemmatization into each word of textdata.
    return textdata 



def createPosting(paths):     #function create posting list.
  doc = 0
  postings = pd.DataFrame()    #dataframe which store at the end each word with thier corresponding document in which that word present.
  fileIdName = []              #list which store each document which thier unique id no.


  for path in paths: 
    file = open(path, 'r', encoding= 'unicode_escape')    
    # text = file.read().strip()
    text = " ".join(file.read().split())    #
    file.close()
    preprocessed_text = preprocess(text)  #perform preprocess of text and store into preprocessed_text.
    
    # preprocessing of each file contain textdata is done till here.
    print(doc)
    
    tokens = word_tokenize(str(preprocessed_text))      #tokenize the preprocessed text and convert into tokens.
    for token in tokens:
      if token in postings:
        p = postings[token][0]       # Retrive Posting List of current Token
        p.add(doc)                   # Add the DocId to the List
        postings[token][0] = p       # Update posting List with current DocID.
      else:
        postings.insert(value=[{doc}], loc=0, column=token)  # Add new word in posting list with current DocID

    filename = os.path.basename(path)    #extract last filename from  the path
    fileIdName.append([filename])        #append into fileIdname.
    doc += 1

  fileIdName = pd.DataFrame(fileIdName)     #convert fileIdName into dataframe.

  # save the posting and fileIdname that can we build futher with run above code.
  postings.to_pickle("posting")
  fileIdName.to_pickle("fileIdName")




# DocIds Not function which is used to find the negotion of DocIds.
def Fnot(X):
    b = set(range(len(paths)))
    return list(b.difference(X))    #this function return not of given set.


def Function_Or(X, Y):      #function perform or operation between two given x and y set.
  print("Function Or")
  n = len(X)     #store length of X and Y into n and m.
  m = len(Y)

  Res = []
  comp =0

  i = 0
  j = 0

  while i<n and j<m:    #perform union in two set X and Y and count number of comparision.
    if X[i]==Y[j]:
      Res.append(X[i])
      i=i+1
      j=j+1
      comp=comp+1
    elif X[i]<Y[j]:
      Res.append(X[i])
      i=i+1
      comp=comp+1
    else:
      Res.append(Y[j])
      j=j+1
      comp=comp+1 


  while i<n:    #when only X set left then append into result without any comparision.
    Res.append(X[i])
    i=i+1

  while j<m:     #when only X set left then append into result without any comparision.
    Res.append(Y[j])
    j=j+1  

  return Res, comp           #return union of X and Y and number of comparision.



def  Function_And(X, Y):       #perform intersection in two set X and Y and count number of comparision.
  print("Function And") 

  n = len(X)            #store length of X and Y into n and m.
  m = len(Y)

  Res = []
  comp =0

  i = 0
  j = 0

  while i<n and j<m:           #perform union in two set X and Y and count number of comparision.
    if X[i]==Y[j]:
      Res.append(X[i])
      i=i+1
      j=j+1
      comp=comp+1
    elif X[i]<Y[j]:
      i=i+1
      comp=comp+1
    else:
      j=j+1
      comp=comp+1

  return Res, comp     #return intersection of X and Y and number of comparision.


  

def Function_Or_Not(X, Y):      #function perform or Not of X and Y.
  print("Function Or Not")
  Y = Fnot(Y)     #find not of Y.
  Res, comp = Function_Or(X, Y)      #after finding not of Y then perform OR of X and Y and find no of comparision.
  return Res, comp                  #return the result after X or Not y and number of comparision.
  

def Function_And_Not(X, Y):
  print("Function And Not")
  Y = Fnot(Y)         #find not of Y.
  Res, comp = Function_And(X, Y)       #after finding not of Y then performand of X and Y and find number of comparision.
  return Res, comp                      #return the result after X or Not y and number of comparision.




#in this method we took query from left to rigth and merge into one set and find number of comparision.
def Method1(Query_docIds):
  op_pos = 0
  Total_comp = 0
  X = Query_docIds[0]
  for operand in Query_docIds[1:]:
    Y =operand

    operator = operations[op_pos]
    op_pos = op_pos+1

    if(operator == 'OR'):      #if operator is OR then call Function_or and find resultant list and number of comparision.
      print('OR')
      X, No_of_compare = Function_Or(X, Y)
      Total_comp = Total_comp + No_of_compare

    elif(operator == 'AND'):       #if operator is AND then call Function_or and find resultant list and number of comparision.
      print('AND')
      X, No_of_compare = Funtion_And(X, Y)
      Total_comp = Total_comp + No_of_compare

    elif(operator == 'OR NOT'):       #if operator is OR NOT then call Function_or and find resultant list and number of comparision.
      print('OR NOT')
      print("varun 1")
      X, No_of_compare = Function_Or_Not(X, Y)
      Total_comp = Total_comp + No_of_compare

    elif(operator == 'AND NOT'):      #if operator is AND NOT then call Function_or and find resultant list and number of comparision.
      print("AND NOT")
      X, No_of_compare = Function_And_Not(X, Y)
      Total_comp = Total_comp + No_of_compare

    else:
      print("You Entered Wrong Operations................!!!")

  print("\n\n")
  print("Input Query: ", Query)
  print("Input operation sequence: ", operations)
  print("Number of documents matched: ", len(X))
  print("No. of comparisons required: ",Total_comp)  

  # print the document names retrieved
  name_list = []
  for i in X:
    name_list.append(fileIdName.iloc[i][0])
  print(name_list)

######################################################################################


def Function_compare_count(X, Y, operator):
  if(operator == 'OR'):
    print('OR')
    X, No_of_compare = Function_Or(X, Y)     #if operator is OR then call Function_or and find resultant list and number of comparision.
    # Total_comp = Total_comp + No_of_compare

  elif(operator == 'AND'):               #if operator is AND then call Function_or and find resultant list and number of comparision.
    print('AND')
    X, No_of_compare = Funtion_And(X, Y)
    # Total_comp = Total_comp + No_of_compare

  elif(operator == 'OR NOT'):          #if operator is OR NOT then call Function_or and find resultant list and number of comparision.
    print('OR NOT')
    print("varun 1")
    X, No_of_compare = Function_Or_Not(X, Y)
    # Total_comp = Total_comp + No_of_compare

  elif(operator == 'AND NOT'):         #if operator is And NOT then call Function_or and find resultant list and number of comparision.
    print("AND NOT")
    X, No_of_compare = Function_And_Not(X, Y)
    # Total_comp = Total_comp + No_of_compare

  return X, No_of_compare 


########################################################################################
#this method count minimum number of comparision by picking smallest set and perform operation with one of his neighbour.
def Method2(Query_docIds):

  # print(Query_docIds)
  Query_docIds_length = []
  for q in Query_docIds:
    Query_docIds_length.append(len(q))

  print(Query_docIds_length)

  Total_comp = 0
  no_of_op   = len(operations)
  for i in range(1, no_of_op+1):
    
    min_index = Query_docIds_length.index(min(Query_docIds_length))       #find index of min docIds length.
    if(min_index==0):          #if it is first index then perform operation of minimum index docId with his rigth neighbour.
      print("left not exist")
      X = Query_docIds[min_index]     #find min index docid.
      Y = Query_docIds.pop(min_index+1) #find min id docid rigth neighbour.
      Query_docIds_length.pop(min_index+1)

      operator = operations[min_index]

      X,comp = Function_compare_count(X, Y, operator)   #find number of comparision and resultant list after performing pass operation with X and Y

      Query_docIds[min_index] = X     #store result in min index.
      Query_docIds_length[min_index] = len(X)

      operations.pop(min_index)

    else:             #otherwise then perform operation of minimum index docId with his left neighbour.
      print("left case exist")
      Y = Query_docIds.pop(min_index)
      Query_docIds_length.pop(min_index)

      X = Query_docIds[min_index-1]        #find min index docid.
      operator = operations[min_index-1]   #find min id docid left neighbour.

      X, comp = Function_compare_count(X, Y, operator)     #find number of comparision and resultant list after performing pass operation with X and Y

      Query_docIds[min_index-1] = X     #store result in min index leftf neighbour.
      Query_docIds_length[min_index-1] = len(X)

      operations.pop(min_index-1)

    Total_comp = Total_comp + comp      #find total nuber of comparision.

  print("\n\n")
  print("Input Query: ", Query)
  print("Input operation sequence: ", copy_operation)
  print("Number of documents matched: ", len(X))
  print("No. of comparisons required: ",Total_comp)
  
  # print the document names retrieved
  name_list = []
  for i in X:
    name_list.append(fileIdName.iloc[i][0])
  print(name_list)





if __name__ == "__main__":

  # assign stories folder into title
  title = "/content/drive/MyDrive/IR_CSE508/Assignment1/stories"

  # find all files those are stored into title and store into path.
  paths = []
  for (dirpath, dirnames, filenames) in os.walk(str(title)):
    for i in filenames:
      paths.append(str(dirpath)+str("/")+i)


  #  find length of path
  print(len(paths))

  # print first two file path
  print(paths[0])
  print(paths[1])

  ##############################################################################
  # creating a dataframe and list where fieldName store doc name with unique index and posting contain the list of doc id for each unique word.    
  doc = 0
  postings = pd.DataFrame()
  fileIdName = []


  ##############################################################################
  # 
  # createPosting(paths)

  
  ##############################################################################
  # read the posting and fileIdName 
  postings = pd.read_pickle("posting")
  fileIdName = pd.read_pickle("fileIdName")
  ##############################################################################

  # Our input query
  Query = "lion stood thoughtfully for a moment"
  operations = ['OR','OR', 'OR']

  # Query = "telephone,paved, roads"
  # operations = ['OR NOT','AND NOT']
  copy_operation = operations.copy()  # For further printing.

  ##############################################################################
  # Query Pre-processing
  pre_processed_query = preprocess(Query)
  pre_processed_query = pre_processed_query.flatten()
  pre_processed_query = np.char.split(pre_processed_query[0])
  pre_processed_query = pre_processed_query.tolist()
  ##############################################################################

  ##############################################################################
  #print query unique word id in sorted order.
  Query_docIds = []
  for r in pre_processed_query:
    word_docIds = (postings[r][0])
    Query_docIds.append(sorted(word_docIds))
  
  print("\n Printing Query unique words Ids : ")
  print(Query_docIds)
  


  ##############################################################################
  if(len(Query_docIds)-1 == len(operations)):
      # Normal Left to right
      Method1(Query_docIds);

      # More Query Optimize
      # Method2(Query_docIds);   
  
  else:
    print("Enter No Operation is Wrong")

  

  ############################ Thank You #######################################